{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarks\n",
    "\n",
    "## Initialization and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sknetwork.clustering import BiLouvain\n",
    "from sknetwork.embedding import RandomProjection, HLouvainEmbedding, SVD\n",
    "from sknetwork.data import load_netset\n",
    "from sknetwork.topology import connected_components\n",
    "from sknetwork.utils.timeout import TimeOut\n",
    "\n",
    "from nodevectors import ProNE, GGVec\n",
    "\n",
    "from time import time\n",
    "import pickle\n",
    "from itertools import count, filterfalse\n",
    "from pprint import pprint\n",
    "import gc\n",
    "\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster NE implementation\n",
    "from typing import Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sknetwork.clustering.louvain import BiLouvain\n",
    "from sknetwork.embedding.base import BaseBiEmbedding\n",
    "from sknetwork.linalg.normalization import normalize\n",
    "from sknetwork.utils.check import check_random_state, check_adjacency_vector, check_nonnegative\n",
    "from sknetwork.utils.membership import membership_matrix\n",
    "\n",
    "class ClusterNE(BaseBiEmbedding):\n",
    "    \"\"\"Embedding of bipartite graphs induced by Louvain clustering. Each component of the embedding corresponds\n",
    "    to a cluster obtained by Louvain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    resolution : float\n",
    "        Resolution parameter.\n",
    "    modularity : str\n",
    "        Which objective function to maximize. Can be ``'dugue'``, ``'newman'`` or ``'potts'``.\n",
    "    tol_optimization :\n",
    "        Minimum increase in the objective function to enter a new optimization pass.\n",
    "    tol_aggregation :\n",
    "        Minimum increase in the objective function to enter a new aggregation pass.\n",
    "    n_aggregations :\n",
    "        Maximum number of aggregations.\n",
    "        A negative value is interpreted as no limit.\n",
    "    shuffle_nodes :\n",
    "        Enables node shuffling before optimization.\n",
    "    random_state :\n",
    "        Random number generator or random seed. If ``None``, numpy.random is used.\n",
    "    isolated_nodes : str\n",
    "        What to do with isolated column nodes. Can be ``'remove'`` (default), ``'merge'`` or ``'keep'``.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    embedding_ : array, shape = (n, n_components)\n",
    "        Embedding of the nodes.\n",
    "    embedding_row_ : array, shape = (n_row, n_components)\n",
    "        Embedding of the rows (copy of **embedding_**).\n",
    "    embedding_col_ : array, shape = (n_col, n_components)\n",
    "        Embedding of the columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, resolution: float = 1, modularity: str = 'dugue', tol_optimization: float = 1e-3,\n",
    "                 tol_aggregation: float = 1e-3, n_aggregations: int = -1, shuffle_nodes: bool = False,\n",
    "                 random_state: Optional[Union[np.random.RandomState, int]] = None, isolated_nodes: str = 'remove'):\n",
    "        super(ClusterNE, self).__init__()\n",
    "        self.resolution = np.float32(resolution)\n",
    "        self.modularity = modularity.lower()\n",
    "        self.tol_optimization = np.float32(tol_optimization)\n",
    "        self.tol_aggregation = tol_aggregation\n",
    "        self.n_aggregations = n_aggregations\n",
    "        self.shuffle_nodes = shuffle_nodes\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.isolated_nodes = isolated_nodes\n",
    "\n",
    "        self.labels_ = None\n",
    "\n",
    "    def fit(self, biadjacency: sparse.csr_matrix):\n",
    "        \"\"\"Embedding of bipartite graphs from the clustering obtained with Louvain.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        biadjacency:\n",
    "            Biadjacency matrix of the graph.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self: :class:`BiLouvainEmbedding`\n",
    "        \"\"\"\n",
    "        bilouvain = BiLouvain(resolution=self.resolution, modularity=self.modularity,\n",
    "                              tol_optimization=self.tol_optimization, tol_aggregation=self.tol_aggregation,\n",
    "                              n_aggregations=self.n_aggregations, shuffle_nodes=self.shuffle_nodes, sort_clusters=False,\n",
    "                              return_membership=True, return_aggregate=True, random_state=self.random_state)\n",
    "        bilouvain.fit(biadjacency)\n",
    "\n",
    "        self.labels_ = bilouvain.labels_\n",
    "\n",
    "        embedding_row = bilouvain.membership_row_\n",
    "        embedding_col = bilouvain.membership_col_\n",
    "\n",
    "        if self.isolated_nodes in ['remove', 'merge']:\n",
    "            # remove or merge isolated column nodes and reindex labels\n",
    "            labels_unique, counts = np.unique(bilouvain.labels_col_, return_counts=True)\n",
    "            n_labels = max(labels_unique) + 1\n",
    "            labels_old = labels_unique[counts > 1]\n",
    "            if self.isolated_nodes == 'remove':\n",
    "                labels_new = -np.ones(n_labels, dtype='int')\n",
    "            else:\n",
    "                labels_new = len(labels_old) * np.ones(n_labels, dtype='int')\n",
    "            labels_new[labels_old] = np.arange(len(labels_old))\n",
    "            labels_col = labels_new[bilouvain.labels_col_]\n",
    "\n",
    "            # reindex row labels accordingly\n",
    "            labels_unique = np.unique(bilouvain.labels_row_)\n",
    "            n_labels = max(labels_unique) + 1\n",
    "            labels_new = -np.ones(n_labels, dtype='int')\n",
    "            labels_new[labels_old] = np.arange(len(labels_old))\n",
    "            labels_row = labels_new[bilouvain.labels_row_]\n",
    "\n",
    "            # get embeddings\n",
    "            probs = normalize(biadjacency)\n",
    "            embedding_row = probs.dot(membership_matrix(labels_col))\n",
    "            probs = normalize(biadjacency.T)\n",
    "            embedding_col = probs.dot(membership_matrix(labels_row))\n",
    "\n",
    "        self.embedding_row_ = embedding_row.toarray()\n",
    "        self.embedding_col_ = embedding_col.toarray()\n",
    "        self.embedding_ = self.embedding_row_\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset selection\n",
    "graph = load_netset('polblogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Datasets statistics\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of labels\n",
    "len(set(graph.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-dimension setting\n",
    "\n",
    "cne = ClusterNE()\n",
    "N_COMPONENTS = cne.fit_transform(graph.adjacency).shape[1]\n",
    "print(N_COMPONENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms under study\n",
    "methods = {\n",
    "           'clusterNE': ClusterNE,\n",
    "           'NMF' : NMF,\n",
    "           'SVD' : SVD,\n",
    "           'randNE': RandomProjection,\n",
    "           'proNE': ProNE,\n",
    "           'louvainNE': HLouvainEmbedding,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing algorithms\n",
    "algorithms = {}\n",
    "embeddings = {}\n",
    "variant_name = f'Dim. {N_COMPONENTS}/Res. 1'\n",
    "algorithms[variant_name] = {}\n",
    "embeddings[variant_name] = {}\n",
    "for name in methods:\n",
    "    sample = methods[name]()\n",
    "    if hasattr(sample, 'n_components'):\n",
    "        if hasattr(sample, 'mu'):\n",
    "            algorithms[variant_name][name] = methods[name](n_components=N_COMPONENTS, mu=.1)\n",
    "        else:\n",
    "            algorithms[variant_name][name] = methods[name](n_components=N_COMPONENTS)\n",
    "    if hasattr(sample, 'resolution'):\n",
    "        algorithms[variant_name][name] = methods[name]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating embeddings\n",
    "for param_name in algorithms:\n",
    "    times[param_name] = {}\n",
    "    for name in algorithms[param_name]:\n",
    "        dep = time()\n",
    "        embeddings[param_name][name] = algorithms[param_name][name].fit_transform(graph.adjacency)\n",
    "        times[param_name][name] = time() - dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution times\n",
    "pd.DataFrame(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link prediction: AUC on negative vs positive sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLES = 10000\n",
    "samples = np.zeros((2*N_SAMPLES, 3), dtype=int)\n",
    "degrees = graph.adjacency.dot(np.ones(graph.adjacency.shape[0], dtype=int))\n",
    "nodes = np.random.choice(np.arange(graph.adjacency.shape[0], dtype=int)[degrees>0], size = N_SAMPLES)\n",
    "indptr, indices = graph.adjacency.indptr, graph.adjacency.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(N_SAMPLES):\n",
    "    node = nodes[index]\n",
    "    pos_samp = np.random.choice(indices[indptr[node]:indptr[node+1]])\n",
    "    neg_samp = next(filterfalse(set(indices[indptr[node]:indptr[node+1]]).__contains__, count(1)))\n",
    "    samples[2*index,:] = node, pos_samp, 1\n",
    "    samples[2*index + 1,:] = node, neg_samp, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in algorithms:\n",
    "    auc_scores[param_name] = {}\n",
    "    for name in algorithms[param_name]:\n",
    "        similarities = cosine_similarity(embeddings[param_name][name][samples[:,0]],\n",
    "                                         embeddings[param_name][name][samples[:,1]])\n",
    "        auc_scores[param_name][name] = roc_auc_score(samples[:,2], np.diagonal(similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classif: compare ground truth against KMeans on the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in algorithms:\n",
    "    knn_scores[param_name] = {}\n",
    "    for name in algorithms[param_name]:\n",
    "        knn_scores[param_name][name] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 3\n",
    "for i in range(N_RUNS):\n",
    "    print('Run', i)\n",
    "    # Drawing samples\n",
    "    N_TRAINING_PER_CLASS = 100\n",
    "    n_classes = len(set(graph.labels))\n",
    "    n_tot = graph.adjacency.shape[0] - N_TRAINING_PER_CLASS * n_classes\n",
    "    samples = np.zeros(N_TRAINING_PER_CLASS * n_classes, dtype=int)\n",
    "    for cl in range(n_classes):\n",
    "        mask = (graph.labels == cl)\n",
    "        samples[cl * N_TRAINING_PER_CLASS: (cl + 1) * N_TRAINING_PER_CLASS] = np.random.choice(np.arange(len(graph.labels))[mask], N_TRAINING_PER_CLASS)\n",
    "    complement = np.array(list(set(np.arange(graph.adjacency.shape[0])) - set(samples)))\n",
    "    \n",
    "    for param_name in algorithms:\n",
    "        for name in algorithms[param_name]:\n",
    "            neigh = SVC()\n",
    "            neigh.fit(embeddings[param_name][name][samples], graph.labels[samples])\n",
    "            labels = neigh.predict(embeddings[param_name][name][complement])\n",
    "            knn_scores[param_name][name] += (graph.labels[complement] == labels).sum() / n_tot\n",
    "            \n",
    "for param_name in algorithms:\n",
    "    for name in algorithms[param_name]:\n",
    "        knn_scores[param_name][name] /= N_RUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(knn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjacency contained in `wdc_hyperlink.npz` should be a subgraph of the [Web Data Commons crawl of the web](http://webdatacommons.org/hyperlinkgraph/). It can be obtained by parsing the first few parts (~ 30) of the edgelist by using `sknetwork.data.load_edge_list` and summing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = sparse.load_npz('wdc_hyperlink.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {}\n",
    "TIMEOUT = 7200\n",
    "key = list(algorithms.keys()).pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sizes of the subgraph to consider to obtain the number of edges given in the article\n",
    "sizes = [int(el) for el in [3e6, 6e6, 1.4e7]]\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    # Slicing to the desired size\n",
    "    small_adj = adj[:int(size), :][:, :int(size)]\n",
    "    # Keeping the largest connected component only\n",
    "    res = connected_components(small_adj)\n",
    "    _, counts = np.unique(res, return_counts=True)\n",
    "    top = np.argmax(counts)\n",
    "    small_adj = small_adj[res == top, :][:, res == top]\n",
    "    print('Sliced:', repr(small_adj))\n",
    "    # ClusterNE goes first to set the dimension\n",
    "    dep = time()\n",
    "    try:\n",
    "        with TimeOut(TIMEOUT):\n",
    "            algorithms[key]['clusterNE'].fit_transform(small_adj)\n",
    "        times['clusterNE'] = time() - dep\n",
    "        print(key,'-','clusterNE',':',algorithms[key]['clusterNE'].embedding_.shape[1])\n",
    "        DIM = algorithms[key]['clusterNE'].embedding_.shape[1]\n",
    "    except TimeoutError:\n",
    "        times['clusterNE'] = f'Timeout (exceeded {TIMEOUT}s)'\n",
    "    with open(\"times-%.1e.p\" % size, 'wb') as file:\n",
    "        pickle.dump(times, file)\n",
    "    # Free the RAM for the next algo\n",
    "    algorithms[key]['clusterNE'].embedding_ = None\n",
    "    gc.collect()\n",
    "    for name in algorithms[key]:\n",
    "        print('Running', name)\n",
    "        if name != 'clusterNE':\n",
    "            algorithms[key][name].n_components = DIM\n",
    "            dep = time()\n",
    "            try:\n",
    "                with TimeOut(TIMEOUT):\n",
    "                    algorithms[key][name].fit_transform(small_adj)\n",
    "                times[name] = time() - dep\n",
    "            except TimeoutError:\n",
    "                times[name] = f'Timeout (exceeded {TIMEOUT}s)'\n",
    "            # Saving the results\n",
    "            with open(\"times-%.1e.p\" % size, 'wb') as file:\n",
    "                pickle.dump(times, file)\n",
    "            # Free the RAM for the next algo\n",
    "            algorithms[key][name].embedding_ = None\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in sizes:\n",
    "    print('\\nSize', size)\n",
    "    with open('times-%.1e.p' % size, 'rb') as file:\n",
    "        dico = pickle.load(file)\n",
    "        for entry in dico:\n",
    "            print(entry, ':', dico[entry])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
